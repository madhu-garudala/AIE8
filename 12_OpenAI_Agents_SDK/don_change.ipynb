{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"What would you like to research? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANNER_PROMPT = (\n",
    "    \"You are a helpful research assistant. Given a query, come up with a set of web searches to perform\" \n",
    "    \"to best answer the query. Output between 5 and 20 terms to query for.\"\n",
    ")\n",
    "\n",
    "SEARCH_PROMPT = (\n",
    "    \"You are a research assistant. Given a search term, you search the web for that term and\"\n",
    "    \"produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300\"\n",
    "    \"words. Capture the main points. Write succinctly, no need to have complete sentences or good\"\n",
    "    \"grammar. This will be consumed by someone synthesizing a report, so its vital you capture the\"\n",
    "    \"essence and ignore any fluff. Do not include any additional commentary other than the summary\"\n",
    "    \"itself.\"\n",
    ")\n",
    "\n",
    "WRITER_PROMPT = (\n",
    "    \"You are a senior researcher tasked with writing a cohesive report for a research query. \"\n",
    "    \"You will be provided with the original query, and some initial research done by a research \"\n",
    "    \"assistant.\\n\"\n",
    "    \"You should first come up with an outline for the report that describes the structure and \"\n",
    "    \"flow of the report. Then, generate the report and return that as your final output.\\n\"\n",
    "    \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
    "    \"for 5-10 pages of content, at least 1000 words.\\n\"\n",
    "    \"For the follow-up questions, provide exactly 5 unique questions that would help extend \"\n",
    "    \"this research. Do not repeat questions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# Standard library\n",
    "import asyncio\n",
    "import getpass\n",
    "import os\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "# Third-party\n",
    "from pydantic import BaseModel\n",
    "from rich.console import Console, Group\n",
    "from rich.live import Live\n",
    "from rich.spinner import Spinner\n",
    "\n",
    "# Local application\n",
    "from agents import Agent, Runner, WebSearchTool, custom_span, gen_trace_id, trace\n",
    "from agents.model_settings import ModelSettings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI API key from the user, with a prompt\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Printer:\n",
    "    def __init__(self, console: Console):\n",
    "        self.live = Live(console=console)\n",
    "        self.items: dict[str, tuple[str, bool]] = {}\n",
    "        self.hide_done_ids: set[str] = set()\n",
    "        self.live.start()\n",
    "\n",
    "    def end(self) -> None:\n",
    "        self.live.stop()\n",
    "\n",
    "    def hide_done_checkmark(self, item_id: str) -> None:\n",
    "        self.hide_done_ids.add(item_id)\n",
    "\n",
    "    def update_item(\n",
    "        self, item_id: str, content: str, is_done: bool = False, hide_checkmark: bool = False\n",
    "    ) -> None:\n",
    "        self.items[item_id] = (content, is_done)\n",
    "        if hide_checkmark:\n",
    "            self.hide_done_ids.add(item_id)\n",
    "        self.flush()\n",
    "\n",
    "    def mark_item_done(self, item_id: str) -> None:\n",
    "        self.items[item_id] = (self.items[item_id][0], True)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        renderables: list[Any] = []\n",
    "        for item_id, (content, is_done) in self.items.items():\n",
    "            if is_done:\n",
    "                prefix = \"✅ \" if item_id not in self.hide_done_ids else \"\"\n",
    "                renderables.append(prefix + content)\n",
    "            else:\n",
    "                renderables.append(Spinner(\"dots\", text=content))\n",
    "        self.live.update(Group(*renderables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchItem(BaseModel):\n",
    "    reason: str\n",
    "    \"Your reasoning for why this search is important to the query.\"\n",
    "\n",
    "    query: str\n",
    "    \"The search term to use for the web search.\"\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: list[WebSearchItem]\n",
    "    \"\"\"A list of web searches to perform to best answer the query.\"\"\"\n",
    "\n",
    "class ReportData(BaseModel):\n",
    "    short_summary: str\n",
    "    \"\"\"A short 2-3 sentence summary of the findings.\"\"\"\n",
    "\n",
    "    markdown_report: str\n",
    "    \"\"\"The final report\"\"\"\n",
    "\n",
    "    follow_up_questions: list[str]\n",
    "    \"\"\"Suggested topics to research further\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_PROMPT,\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=WebSearchPlan,\n",
    ")\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_PROMPT,\n",
    "    tools=[WebSearchTool()],\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_PROMPT,\n",
    "    model=\"o3-mini\",\n",
    "    output_type=ReportData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchManager:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.printer = Printer(self.console)\n",
    "\n",
    "    async def run(self, query: str) -> None:\n",
    "        trace_id = gen_trace_id()\n",
    "        with trace(\"Research trace\", trace_id=trace_id):\n",
    "            self.printer.update_item(\n",
    "                \"trace_id\",\n",
    "                f\"View trace: https://platform.openai.com/traces/trace?trace_id={trace_id}\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "\n",
    "            self.printer.update_item(\n",
    "                \"starting\",\n",
    "                \"Starting research...\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "            search_plan = await self._plan_searches(query)\n",
    "            search_results = await self._perform_searches(search_plan)\n",
    "            report = await self._write_report(query, search_results)\n",
    "\n",
    "            final_report = f\"Report summary\\n\\n{report.short_summary}\"\n",
    "            self.printer.update_item(\"final_report\", final_report, is_done=True)\n",
    "\n",
    "            self.printer.end()\n",
    "\n",
    "        print(\"\\n\\n=====REPORT=====\\n\\n\")\n",
    "        print(f\"Report: {report.markdown_report}\")\n",
    "        print(\"\\n\\n=====FOLLOW UP QUESTIONS=====\\n\\n\")\n",
    "        unique_questions = []\n",
    "        seen = set()\n",
    "        \n",
    "        for question in report.follow_up_questions:\n",
    "            if question not in seen:\n",
    "                unique_questions.append(question)\n",
    "                seen.add(question)\n",
    "        \n",
    "        for i, question in enumerate(unique_questions, 1):\n",
    "            print(f\"{i}. {question}\")\n",
    "\n",
    "    async def _plan_searches(self, query: str) -> WebSearchPlan:\n",
    "        self.printer.update_item(\"planning\", \"Planning searches...\")\n",
    "        result = await Runner.run(\n",
    "            planner_agent,\n",
    "            f\"Query: {query}\",\n",
    "        )\n",
    "        self.printer.update_item(\n",
    "            \"planning\",\n",
    "            f\"Will perform {len(result.final_output.searches)} searches\",\n",
    "            is_done=True,\n",
    "        )\n",
    "        return result.final_output_as(WebSearchPlan)\n",
    "\n",
    "    async def _perform_searches(self, search_plan: WebSearchPlan) -> list[str]:\n",
    "        with custom_span(\"Search the web\"):\n",
    "            self.printer.update_item(\"searching\", \"Searching...\")\n",
    "            num_completed = 0\n",
    "            max_concurrent = 5\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(search_plan.searches), max_concurrent):\n",
    "                batch = search_plan.searches[i:i+max_concurrent]\n",
    "                tasks = [asyncio.create_task(self._search(item)) for item in batch]\n",
    "                \n",
    "                for task in asyncio.as_completed(tasks):\n",
    "                    try:\n",
    "                        result = await task\n",
    "                        if result is not None:\n",
    "                            results.append(result)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Search error: {e}\")\n",
    "                        \n",
    "                    num_completed += 1\n",
    "                    self.printer.update_item(\n",
    "                        \"searching\", f\"Searching... {num_completed}/{len(search_plan.searches)} completed\"\n",
    "                    )\n",
    "            \n",
    "            self.printer.mark_item_done(\"searching\")\n",
    "            return results\n",
    "\n",
    "    async def _search(self, item: WebSearchItem) -> str | None:\n",
    "        input = f\"Search term: {item.query}\\nReason for searching: {item.reason}\"\n",
    "        try:\n",
    "            result = await Runner.run(\n",
    "                search_agent,\n",
    "                input,\n",
    "            )\n",
    "            return str(result.final_output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{item.query}': {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _write_report(self, query: str, search_results: list[str]) -> ReportData:\n",
    "        self.printer.update_item(\"writing\", \"Thinking about report...\")\n",
    "        input = f\"Original query: {query}\\nSummarized search results: {search_results}\"\n",
    "        \n",
    "        result = Runner.run_streamed(\n",
    "            writer_agent,\n",
    "            input,\n",
    "        )\n",
    "        \n",
    "        update_messages = [\n",
    "            \"Thinking about report...\",\n",
    "            \"Planning report structure...\",\n",
    "            \"Writing outline...\",\n",
    "            \"Creating sections...\",\n",
    "            \"Cleaning up formatting...\",\n",
    "            \"Finalizing report...\",\n",
    "            \"Finishing report...\",\n",
    "        ]\n",
    "\n",
    "        last_update = time.time()\n",
    "        next_message = 0\n",
    "        \n",
    "        async for event in result.stream_events():\n",
    "            if time.time() - last_update > 5 and next_message < len(update_messages):\n",
    "                self.printer.update_item(\"writing\", update_messages[next_message])\n",
    "                next_message += 1\n",
    "                last_update = time.time()\n",
    "\n",
    "        self.printer.mark_item_done(\"writing\")\n",
    "        return result.final_output_as(ReportData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "    await ResearchManager().run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca22ccd9a5b246f28939b47ecca3b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====REPORT=====\n",
      "\n",
      "\n",
      "Report: # Research Report on RAGAS: An In-Depth Exploration\n",
      "\n",
      "This report investigates two distinct areas associated with the term “RAGAS”. On one hand, it delves into the world of Retrieval Augmented Generation (RAG) evaluation through the RAGAS framework—an automated, reference-free system designed to enhance the evaluation of large language model (LLM) applications. On the other, it examines the traditional and contemporary research surrounding Indian classical ragas, exploring their socio-cultural, emotional, and therapeutic significance. The following pages provide a detailed account of both themes, outlining methodologies, metrics, historical evolution, practical applications, and future research directions.\n",
      "\n",
      "---\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "1. [Introduction](#introduction)\n",
      "2. [RAGAS in the Context of AI Evaluation](#ragas-in-ai)\n",
      "   - 2.1 [Background: Retrieval Augmented Generation (RAG)](#background)\n",
      "   - 2.2 [Overview of the RAGAS Framework](#ragas-framework)\n",
      "   - 2.3 [Key Metrics and Evaluation Methodologies](#metrics)\n",
      "   - 2.4 [Comparative Analysis with Related Frameworks](#comparative-analysis)\n",
      "3. [Indian Classical Ragas: Tradition and Research](#indian-ragas)\n",
      "   - 3.1 [Historical Evolution and Cultural Significance](#historical-evolution)\n",
      "   - 3.2 [Therapeutic Effects and Emotional Impact](#therapeutic-effects)\n",
      "   - 3.3 [Contemporary Research and Modern Integration](#modern-integration)\n",
      "4. [Community Impact and Broader Applications](#community-impact)\n",
      "5. [Conclusion and Future Directions](#conclusion)\n",
      "\n",
      "---\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In today’s rapidly evolving world, the term RAGAS serves as an umbrella for diverse research fields. The first component of this report focuses on a technological paradigm designed to evaluate advanced AI systems—specifically, Retrieval Augmented Generation (RAG) models. These models merge the robustness of large language models with retrieval systems, achieving more accurate and context-specific outputs. The RAGAS (Retrieval Augmented Generation Assessment) framework has emerged as an essential tool to perform objective, rapid, and automated evaluations of RAG systems without the heavy reliance on human annotations.\n",
      "\n",
      "The second component focuses on the rich musical tradition of Indian classical ragas. Ragas are more than mere musical scales; they embody a deep cultural heritage, reflecting centuries of evolution in musical thought and technique. They are studied not only for their beauty and cultural significance but also for their purported therapeutic effects on mental and physical health.\n",
      "\n",
      "This dual-focused report underscores that, while these subjects occupy different disciplinary spaces, both represent areas of innovation and cultural significance that continue to evolve with time.\n",
      "\n",
      "---\n",
      "\n",
      "## RAGAS in the Context of AI Evaluation\n",
      "\n",
      "### Background: Retrieval Augmented Generation (RAG)\n",
      "\n",
      "Retrieval Augmented Generation (RAG) models have revolutionized the approach to natural language processing tasks. Traditional LLMs generate responses based solely on the parameters learned during training. In contrast, RAG models integrate external information retrieval, which allows the system to query a large corpus—such as Wikipedia—for relevant information. This integration leads to better accuracy, diversity, and factual grounding in responses. However, the hybrid nature of retrieval and generation necessitates new evaluation measures.\n",
      "\n",
      "### Overview of the RAGAS Framework\n",
      "\n",
      "The RAGAS framework is an open-source project tailored for the automated, reference-free evaluation of RAG systems. It addresses multiple key challenges faced by RAG systems:\n",
      "\n",
      "- **Automated Evaluation Metrics**: RAGAS uses industry-standard, automated metrics to assess the quality, reliability, and performance of RAG applications. These include metrics related to context relevance, answer faithfulness, and overall generation accuracy.\n",
      "\n",
      "- **Synthetic Evaluation Data Generation**: The framework is capable of generating synthetic data that simulates real-world user queries and retrieval contexts. This feature allows for comprehensive testing that covers diverse scenarios.\n",
      "\n",
      "- **Online Monitoring and Continuous Feedback**: RAGAS is designed to work not only in lab settings but also in production environments, providing real-time insights and diagnostics. This enables continuous quality monitoring and iterative improvements.\n",
      "\n",
      "### Key Metrics and Evaluation Methodologies\n",
      "\n",
      "A major strength of the RAGAS framework lies in its multifaceted approach to evaluation. It categorizes metrics on various dimensions:\n",
      "\n",
      "1. **Context Relevance and Precision**: This metric assesses how well the system identifies and retrieves relevant passages from reference databases. It examines both the precision (correctness of retrieved context) and recall (exhaustiveness in retrieving all relevant passages).\n",
      "\n",
      "2. **Answer Relevance and Faithfulness**: Once context is retrieved, it is imperative that the generated answer relies on the provided context. Faithfulness measures the tendency of the LLM to adhere to the factual details from the retrieved passages, while answer relevance ensures that the response directly addresses the user query.\n",
      "\n",
      "3. **LLM-based vs. Non-LLM-based Metrics**: The framework also distinguishes between LLM-based metrics (which are more dynamic and closer to human judgement but introduce non-determinism) and non-LLM-based metrics (like string similarity and BLEU scores) that offer deterministic evaluations.\n",
      "\n",
      "4. **Additional Metrics**: Other metrics assess noise sensitivity (the system’s robustness to irrelevant data), discrete vs. numerical outputs, and ranking metrics that compare multiple outputs simultaneously.\n",
      "\n",
      "### Comparative Analysis with Related Frameworks\n",
      "\n",
      "RAGAS is not the only framework in this niche. Other notable systems include ARES, MIRAGE, THELMA, and FATHOMS-RAG. While similar in their objectives—to evaluate the integration of retrieval and generation components—each framework offers unique approaches:\n",
      "\n",
      "- **ARES**: Emphasizes automated evaluation using synthetic training data to fine-tune lightweight language model judges.\n",
      "\n",
      "- **MIRAGE and THELMA**: These frameworks provide comprehensive benchmarks and reference-free evaluation methods, albeit with different metric focuses and workflow optimizations.\n",
      "\n",
      "- **FATHOMS-RAG**: Specifically targets multimodal evaluation, including text, tables, and images, thus extending the evaluation to more varied content spaces.\n",
      "\n",
      "The competitive landscape underlines an ongoing evolution in the methodologies and mandates for RAG evaluation, with RAGAS standing out for its comprehensive suite of metrics and integration with popular AI frameworks like LangChain and LlamaIndex.\n",
      "\n",
      "---\n",
      "\n",
      "## Indian Classical Ragas: Tradition and Research\n",
      "\n",
      "While technological innovation unfolds in the realm of artificial intelligence, Indian classical music remains a domain of profound cultural resonance. Indian ragas have traversed centuries, evolving through numerous stylistic and performance paradigms.\n",
      "\n",
      "### Historical Evolution and Cultural Significance\n",
      "\n",
      "Indian classical ragas are rooted deeply in tradition. Historically, they emerged from rich oral traditions and evolved through structured performance techniques such as Dhrupad and Khayal. Over the 18th century and beyond, ragas reflected shifting social and cultural narratives. For instance, the evolution from the disciplined forms of Dhrupad to the more expressive Khayal and Thumri forms has been well documented by musicologists. This evolution not only mirrors changes in political and cultural climates but also signifies the adaptability and dynamic nature of Indian art forms.\n",
      "\n",
      "Furthermore, the cultural significance of ragas is profound. They are imbued with philosophical meaning, where each raga is associated with specific times of day, seasons, and emotional nuances. This multifaceted use—ranging from ceremonial purposes to personal introspection—continues to anchor ragas as an integral part of Indian heritage.\n",
      "\n",
      "### Therapeutic Effects and Emotional Impact\n",
      "\n",
      "Modern research has revisited the therapeutic potential of Indian classical ragas. Studies published in reputable journals have explored how specific ragas such as Darbari, Bhairav, and Malkauns can influence brain wave patterns, enhance hormonal balance, and decrease stress levels. There is growing evidence that listening to these ragas may help in emotional regulation and even offer benefits for cardiovascular health.\n",
      "\n",
      "Moreover, experiments involving EEG patterns highlight the ability of ragas to modulate brain rhythms, which could be leveraged in developing music-based mental health interventions. The interplay between musical intervals, tempo, and rhythmic regularity, such as the introduction of a ‘minor second’, has been shown to affect listeners’ emotional states in predictable ways.\n",
      "\n",
      "### Contemporary Research and Modern Integration\n",
      "\n",
      "In the contemporary setting, the influence of modern technology has led to intriguing crossovers between traditional music and digital innovation. Online platforms and digital libraries now host extensive recordings and tutorials, making ragas accessible to a global audience. Additionally, community forums and academic initiatives facilitate ongoing discourse and research on the evolution and preservation of these art forms.\n",
      "\n",
      "Festivals and live concerts, such as the annual Ragas Live Festival and events like \"Ragas Reinvented,\" underscore the vitality and continued relevance of these traditions. These events often merge classical performances with modern elements, drawing a diverse audience and encouraging a renewed interest in Indian classical music.\n",
      "\n",
      "---\n",
      "\n",
      "## Community Impact and Broader Applications\n",
      "\n",
      "### RAGAS Framework in the AI Community\n",
      "\n",
      "The open-source nature of the RAGAS framework encourages community contributions from researchers, developers, and practitioners. It offers a platform for sharing insights, troubleshooting challenges, and collaborating on the refinement of automated evaluation metrics. The integration with existing AI ecosystems ensures that improvements in evaluation techniques can be rapidly deployed in production environments, thereby accelerating the development lifecycle of RAG systems.\n",
      "\n",
      "### Cultural Platforms for Indian Ragas\n",
      "\n",
      "Similarly, the world of Indian classical music has benefited immensely from digital community initiatives. Forums, social media groups, and dedicated websites such as RagaForum and Scaler Music Community enable enthusiasts to share research, performance techniques, and experiences. Such platforms not only aid in preserving ancient art forms but also promote new compositions that fuse traditional and modern musical expressions.\n",
      "\n",
      "### Cross-Pollination: Where Technology Meets Tradition\n",
      "\n",
      "Interestingly, there is a fertile ground for cross-disciplinary studies where the methodologies developed in AI evaluation can be applied to the analysis of musical compositions and vice versa. For instance, techniques for quantifying the emotional impact of a musical raga could benefit from data-driven approaches similar to those used in evaluating AI responses. This intersection of tech and tradition invites innovative research projects that span cognitive sciences, digital humanities, and machine learning.\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion and Future Directions\n",
      "\n",
      "The term “RAGAS” encapsulates two vibrant and seemingly disparate yet equally rich areas of study. On one side, the RAGAS framework represents a significant leap in automated, objective evaluation of RAG systems in artificial intelligence. Its comprehensive metrics, synthetic data generation, and real-time monitoring capabilities address critical challenges in deploying robust AI applications. As RAG systems continue to evolve, RAGAS provides an indispensable toolset for ensuring that modern solutions meet high standards of factual accuracy and contextual relevance.\n",
      "\n",
      "On the other side, Indian classical ragas continue to be a living tradition that influences culture, therapy, and art. Modern research confirms the therapeutic and emotional benefits of ragas, while digital platforms help to preserve and disseminate this age-old heritage. Future research in this area might explore more granular effects of specific ragas on neurological health and creative expression, further bridging the interdisciplinary gap between music and modern science.\n",
      "\n",
      "In conclusion, whether viewed through the lens of artificial intelligence or cultural heritage, the explorations of RAGAS are testament to the diversity and dynamism of modern research. Both fields are characterized by continuous innovation and community engagement, promising exciting future developments in their respective arenas.\n",
      "\n",
      "---\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "1. Continued refinement of the RAGAS evaluation framework to accommodate new evaluation metrics and adapt to ever-changing RAG architectures.\n",
      "2. Development of hybrid metrics that merge LLM-based and non-LLM-based evaluations to produce more robust results.\n",
      "3. Expanded interdisciplinary studies to explore the cognitive and therapeutic implications of Indian classical ragas using modern neuroscience techniques.\n",
      "4. Enhanced community platforms for collaborative research, both in AI evaluation methods and in the analysis and preservation of classical music traditions.\n",
      "5. Increased focus on synthesizing artistic and technological insights to promote innovation in fields as diverse as digital humanities, music therapy, and automated system evaluation.\n",
      "\n",
      "Through continued investigation and creative thinking, both the RAGAS framework for AI and the study of Indian classical ragas will undoubtedly contribute to a deeper understanding of their respective fields, opening new avenues for research and collaboration.\n",
      "\n",
      "\n",
      "\n",
      "=====FOLLOW UP QUESTIONS=====\n",
      "\n",
      "\n",
      "1. How can we further improve the integration of synthetic data generation within the RAGAS framework to better simulate real-world queries?\n",
      "2. What methods could be developed to quantify the emotional and neurological impact of specific Indian classical ragas using modern neuroimaging techniques?\n",
      "3. In what ways can LLM-based metrics be enhanced to reduce their non-deterministic behavior while still capturing nuanced human judgments?\n",
      "4. How could cross-disciplinary studies between AI evaluation techniques and music analysis lead to more precise measurement of artistic expression?\n",
      "5. What role can community-driven projects play in further refining both the RAGAS evaluation framework and the documentation/preservation of traditional Indian music?\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
