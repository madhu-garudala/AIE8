{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXW_DgiSebM"
   },
   "source": [
    "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "- ü§ù Breakout Room #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "\n",
    "- ü§ù Breakout Room #2:\n",
    "  1. Evaluating the LangGraph Application with LangSmith\n",
    "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
    "  3. LangGraph for the \"Patterns\" of GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: arxiv in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (2.2.0)\n",
      "Requirement already satisfied: langchain-community in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-openai in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.3.33)\n",
      "Requirement already satisfied: langgraph in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.6.8)\n",
      "Requirement already satisfied: langsmith in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.4.29)\n",
      "Requirement already satisfied: openevals in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.1.0)\n",
      "Requirement already satisfied: tavily-python in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.7.12)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from arxiv) (6.0.12)\n",
      "Requirement already satisfied: requests~=2.32.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from arxiv) (2.32.5)\n",
      "Requirement already satisfied: sgmllib3k in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.3.76)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (3.11.3)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (2.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (0.25.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: arxiv in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (2.2.0)\n",
      "Requirement already satisfied: langchain-community in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-openai in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.3.33)\n",
      "Requirement already satisfied: langgraph in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.6.8)\n",
      "Requirement already satisfied: langsmith in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.4.29)\n",
      "Requirement already satisfied: openevals in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.1.0)\n",
      "Requirement already satisfied: tavily-python in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (0.7.12)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from arxiv) (6.0.12)\n",
      "Requirement already satisfied: requests~=2.32.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from arxiv) (2.32.5)\n",
      "Requirement already satisfied: sgmllib3k in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.3.76)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (3.11.3)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (2.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langsmith) (0.25.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langsmith) (0.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langsmith) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: rich>=13.9.4 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openevals) (14.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: rich>=13.9.4 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from openevals) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from rich>=13.9.4->openevals) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from rich>=13.9.4->openevals) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->openevals) (0.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from rich>=13.9.4->openevals) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from rich>=13.9.4->openevals) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/madhugarudala/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->openevals) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "!pip install arxiv langchain-community langchain-openai langgraph langsmith openevals tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQ3nRAgoF67"
   },
   "source": [
    "# ü§ù Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7pQDUhUnIo8"
   },
   "source": [
    "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
    "\n",
    "### Why Cycles?\n",
    "\n",
    "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
    "\n",
    "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
    "\n",
    "### Why LangGraph?\n",
    "\n",
    "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Task 1:  Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We'll want to set our OpenAI, Tavily, and LangSmith API keys along with our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jkla2fpx28QK",
    "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE8 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6n_Dob2F46"
   },
   "source": [
    "#### üèóÔ∏è Activity #1:\n",
    "\n",
    "Please add the tools to use into our toolbelt.\n",
    "\n",
    "> NOTE: Each tool in our toolbelt should be a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhugarudala/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/p1/p4pcqggn3_99lj76z6kjml440000gn/T/ipykernel_6294/1203815797.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_tool = TavilySearchResults(max_results=5)\n",
      "/var/folders/p1/p4pcqggn3_99lj76z6kjml440000gn/T/ipykernel_6294/1203815797.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_tool = TavilySearchResults(max_results=5)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "tool_belt = [\n",
    "    tavily_tool,\n",
    "    ArxivQueryRun(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "model = model.bind_tools(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERzuGo6W18Lr"
   },
   "source": [
    "#### ‚ùì Question #1:\n",
    "\n",
    "How does the model determine which tool to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "outputs": [],
   "source": [
    "Answer: The model determines which tool to use through OpenAI's function calling API, where each tool's description and schema are provided to the model, allowing it to intelligently select the most appropriate tool(s) based on the user's query and generate structured function calls accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_296Ub96Z_H8"
   },
   "source": [
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "Earlier we used this phrasing:\n",
    "\n",
    "`coordinated multi-actor and stateful applications`\n",
    "\n",
    "So what does that \"stateful\" mean?\n",
    "\n",
    "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
    "\n",
    "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
    "\n",
    "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
    "\n",
    "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
    "\n",
    "1. We initialize our state object:\n",
    "  - `{\"messages\" : []}`\n",
    "2. Our user submits a query to our application.\n",
    "  - New State: `HumanMessage(#1)`\n",
    "  - `{\"messages\" : [HumanMessage(#1)}`\n",
    "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
    "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
    "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
    "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWsMhfO9grLu"
   },
   "source": [
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
    "\n",
    "Let's take a second to refresh ourselves about what a graph is in this context.\n",
    "\n",
    "Graphs, also called networks in some circles, are a collection of connected objects.\n",
    "\n",
    "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
    "\n",
    "Let's look at a simple graph.\n",
    "\n",
    "![image](https://i.imgur.com/2NFLnIc.png)\n",
    "\n",
    "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
    "\n",
    "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
    "\n",
    "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
    "\n",
    "Let's create some nodes and expand on our diagram.\n",
    "\n",
    "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `tool_node` is a node which can call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vF4_lgtmQNo",
    "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1115bc940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8CjRlbVmRpW"
   },
   "source": [
    "Let's look at what we have so far:\n",
    "\n",
    "![image](https://i.imgur.com/md7inqG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGCbaYqRnmiw",
    "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1115bc940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsfGoSpoF9U"
   },
   "source": [
    "![image](https://i.imgur.com/wNixpJe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_pQgHmoW0M"
   },
   "source": [
    "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
    "\n",
    "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
    "\n",
    "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
    "\n",
    "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
    "\n",
    "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BZgb81VQf9o",
    "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1115bc940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  return END\n",
    "\n",
    "uncompiled_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cvhcf4jp0Ce"
   },
   "source": [
    "Let's visualize what this looks like.\n",
    "\n",
    "![image](https://i.imgur.com/8ZNwKI5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCjWJCkrJb9"
   },
   "source": [
    "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvcgbHf1rIXZ",
    "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1115bc940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWDwBQtrw7Z"
   },
   "source": [
    "Let's look at the final visualization.\n",
    "\n",
    "![image](https://i.imgur.com/NWO7usO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqDpErlsCsu"
   },
   "source": [
    "All that's left to do now is to compile our workflow - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "simple_agent_graph = uncompiled_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhNWIwBL1W4Q"
   },
   "source": [
    "#### ‚ùì Question #2:\n",
    "\n",
    "Is there any specific limit to how many times we can cycle?\n",
    "\n",
    "If not, how could we impose a limit to the number of cycles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "swift"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "By default, LangGraph has no built-in limit on cycles, which could lead to infinite loops.\n",
    "However, you can impose limits through several methods:\n",
    "1. Message count limits (checking len(state['messages']))\n",
    "2. Iteration counters in the state\n",
    "3. Time-based limits\n",
    "4. Conditional logic based on task completion\n",
    "5. Cost-based limits\n",
    "The notebook demonstrates this later with a 10-message limit in the helpfulness check implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Technical professionals across various fields are leveraging AI to enhance their work in numerous ways. Here are some key applications:\\n\\n1. **Automation of Repetitive Tasks**: AI tools can automate routine tasks such as data entry, report generation, and system monitoring, allowing professionals to focus on more complex and creative aspects of their work.\\n\\n2. **Data Analysis and Insights**: AI algorithms can analyze large datasets quickly and accurately, identifying patterns and trends that might be missed by human analysts. This is particularly useful in fields like finance, marketing, and healthcare.\\n\\n3. **Predictive Analytics**: Technical professionals use AI to build predictive models that forecast future trends based on historical data. This is valuable in areas such as supply chain management, risk assessment, and customer behavior analysis.\\n\\n4. **Enhanced Decision-Making**: AI systems can provide recommendations based on data analysis, helping professionals make informed decisions. For example, in software development, AI can suggest code optimizations or identify potential bugs.\\n\\n5. **Natural Language Processing (NLP)**: AI-powered NLP tools are used for sentiment analysis, chatbots, and automated customer support, improving communication and customer engagement.\\n\\n6. **Machine Learning for Personalization**: In fields like marketing and e-commerce, AI is used to create personalized experiences for users by analyzing their behavior and preferences.\\n\\n7. **Improved Design and Prototyping**: In engineering and product design, AI can assist in generating design alternatives, optimizing materials, and simulating performance, speeding up the prototyping process.\\n\\n8. **Cybersecurity**: AI is employed to detect anomalies and potential threats in real-time, enhancing the security of systems and networks. It can analyze patterns of behavior to identify potential breaches.\\n\\n9. **Collaboration Tools**: AI-driven collaboration platforms can enhance teamwork by providing insights, managing workflows, and facilitating communication among team members.\\n\\n10. **Continuous Learning and Development**: AI can help professionals stay updated with the latest trends and technologies by curating personalized learning paths and recommending relevant resources.\\n\\n11. **Quality Assurance**: In software development, AI can automate testing processes, ensuring that applications are bug-free and meet quality standards before deployment.\\n\\n12. **Remote Work Optimization**: AI tools can enhance productivity in remote work settings by managing schedules, prioritizing tasks, and facilitating virtual collaboration.\\n\\nBy integrating AI into their workflows, technical professionals can increase efficiency, reduce errors, and drive innovation, ultimately leading to better outcomes in their respective fields.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 498, 'prompt_tokens': 18, 'total_tokens': 516, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CLdJZASux2nMOUYU2Ai7t1z5o9FPX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--65ed5839-674f-4eef-a1b3-5598d4f69bc6-0', usage_metadata={'input_tokens': 18, 'output_tokens': 498, 'total_tokens': 516, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"How are technical professionals using AI to improve their work?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
    "\n",
    "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afv2BuEsV5JG",
    "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='I\\'m unable to browse the internet or access external databases like Arxiv or Tavily. However, I can guide you on how to perform these searches yourself.\\n\\n1. **Searching Arxiv**:\\n   - Go to the [Arxiv website](https://arxiv.org/).\\n   - Use the search bar to enter the title \"A Comprehensive Survey of Deep\" or relevant keywords.\\n   - Look for the paper in the search results and note the authors\\' names.\\n\\n2. **Finding Author Affiliations**:\\n   - Once you have the authors\\' names, you can search for each author individually on Tavily or other professional networking sites like LinkedIn.\\n   - Simply enter the author\\'s name in the search bar of Tavily or LinkedIn to find their current affiliations.\\n\\nIf you need help with specific information or have questions about deep learning topics, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 175, 'prompt_tokens': 37, 'total_tokens': 212, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CLdJqmDpEJYzLOCK7yATt4urkbUDM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9893939c-64b7-4145-9764-0b6ef96badb8-0', usage_metadata={'input_tokens': 37, 'output_tokens': 175, 'total_tokens': 212, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the A Comprehensive Survey of Deep Research paper, then search each of the authors to find out where they work now using Tavily!\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXzDlZVz1Hnf"
   },
   "source": [
    "#### üèóÔ∏è Activity #2:\n",
    "\n",
    "Please write out the steps the agent took to arrive at the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù Breakout Room #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7c8-Uyarh1v"
   },
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV3XeFOT1Sar"
   },
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wruQCuzewUuO"
   },
   "source": [
    "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "orYxBZXSxJjZ",
    "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '\"Deep research\" generally refers to a thorough and comprehensive investigation into a specific topic or field. It involves going beyond surface-level information to uncover detailed insights, data, and understanding. This type of research often includes:\\n\\n1. **Extensive Literature Review**: Analyzing existing studies, articles, and publications to gather background information and identify gaps in knowledge.\\n\\n2. **Data Collection**: Utilizing various methods such as surveys, experiments, interviews, or observational studies to gather primary data.\\n\\n3. **Critical Analysis**: Evaluating and interpreting the data collected to draw meaningful conclusions and insights.\\n\\n4. **Interdisciplinary Approach**: Often, deep research may involve integrating knowledge from multiple disciplines to provide a more holistic understanding of the topic.\\n\\n5. **Longitudinal Studies**: In some cases, deep research may involve studying a subject over an extended period to observe changes and trends.\\n\\n6. **Peer Review and Validation**: Engaging with the academic or professional community to validate findings and ensure the research meets rigorous standards.\\n\\nDeep research is commonly associated with academic and scientific endeavors but can also apply to market research, policy analysis, and other fields where in-depth understanding is crucial.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_inputs(input_object):\n",
    "  return {\"messages\" : [HumanMessage(content=input_object[\"text\"])]}\n",
    "\n",
    "def parse_output(input_state):\n",
    "  return {\"answer\" : input_state[\"messages\"][-1].content}\n",
    "\n",
    "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output\n",
    "\n",
    "agent_chain_with_formatting.invoke({\"text\" : \"What is Deep Research?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9UkCIqkpyZu"
   },
   "source": [
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
    "\n",
    "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
    "\n",
    "```python\n",
    "questions = [\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Who were the main authors on the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' paper?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"Peng\", \"Xu\"]}   \n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Where do the authors of the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' work now?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"Zhejiang\", \"Liberty Mutual\"]}\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfMXF2KAsQxs"
   },
   "source": [
    "#### üèóÔ∏è Activity #3:\n",
    "\n",
    "Please create a dataset in the above format with at least 5 questions that pertain to the cohort use-case (more information [here](https://www.notion.so/Session-4-RAG-with-LangGraph-OSS-Local-Models-Eval-w-LangSmith-26acd547af3d80838d5beba464d7e701#26acd547af3d81d08809c9c82a462bdd)), or the use-case you're hoping to tackle in your Demo Day project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbagRuJop83E"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Who were the main authors on the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' paper?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Peng\", \"Xu\"]}   \n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Where do the authors of the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' work now?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Zhejiang\", \"Liberty Mutual\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"What are the main applications of LangGraph in building AI agents?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"cyclic\", \"stateful\", \"multi-actor\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"How does LangGraph differ from traditional DAG chains?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"cycles\", \"loops\", \"agent-forward\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"What tools are commonly used with LangGraph for research tasks?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Tavily\", \"Arxiv\", \"search\"]}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7QVFuAmsh7L"
   },
   "source": [
    "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLfrZrgSsn85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['922ead04-b155-4cea-aaa3-1fe8a3c5cf4a',\n",
       "  '95ab1fb6-ca89-453a-a63f-2ac44e752a19',\n",
       "  '5fde885e-3fd6-42e6-96f4-cbdbf4cd4686',\n",
       "  'e7bf1c8d-d964-4800-991f-7d56e52e0b44',\n",
       "  '6c40eaa3-63c5-4c4f-9735-ee824902cb4b',\n",
       "  '1d20d11d-8c3b-4959-9b34-1058a71f84df'],\n",
       " 'count': 6}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = f\"Simple Search Agent - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions about the cohort use-case to evaluate the Simple Search Agent.\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=questions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lRTXUrTtP9Y"
   },
   "source": [
    "### Task 2: Adding Evaluators\n",
    "\n",
    "Let's use the OpenEvals library to product an evaluator that we can then pass into LangSmith!\n",
    "\n",
    "> NOTE: Examine the `CORRECTNESS_PROMPT` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
      "\n",
      "<Rubric>\n",
      "  A correct answer:\n",
      "  - Provides accurate and complete information\n",
      "  - Contains no factual errors\n",
      "  - Addresses all parts of the question\n",
      "  - Is logically consistent\n",
      "  - Uses precise and accurate terminology\n",
      "\n",
      "  When scoring, you should penalize:\n",
      "  - Factual errors or inaccuracies\n",
      "  - Incomplete or partial answers\n",
      "  - Misleading or ambiguous statements\n",
      "  - Incorrect terminology\n",
      "  - Logical inconsistencies\n",
      "  - Missing key information\n",
      "</Rubric>\n",
      "\n",
      "<Instructions>\n",
      "  - Carefully read the input and output\n",
      "  - Check for factual accuracy and completeness\n",
      "  - Focus on correctness of information rather than style or verbosity\n",
      "</Instructions>\n",
      "\n",
      "<Reminder>\n",
      "  The goal is to evaluate factual correctness and completeness of the response.\n",
      "</Reminder>\n",
      "\n",
      "<input>\n",
      "{inputs}\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "{outputs}\n",
      "</output>\n",
      "\n",
      "Use the reference outputs below to help you evaluate the correctness of the response:\n",
      "\n",
      "<reference_outputs>\n",
      "{reference_outputs}\n",
      "</reference_outputs>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "print(CORRECTNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrAUXMFftlAY"
   },
   "outputs": [],
   "source": [
    "from openevals.llm import create_llm_as_judge\n",
    "\n",
    "correctness_evaluator = create_llm_as_judge(\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        model=\"openai:o3-mini\", # very impactful to the final score\n",
    "        feedback_key=\"correctness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a custom Evaluator for our created dataset above - we do this by first making a simple Python function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "  # determine if the phrases in the reference_outputs are in the outputs\n",
    "  required = reference_outputs.get(\"must_mention\") or []\n",
    "  score = all(phrase in outputs[\"answer\"] for phrase in required)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNtHORUh0jZY"
   },
   "source": [
    "#### ‚ùì Question #4:\n",
    "\n",
    "What are some ways you could improve this metric as-is?\n",
    "\n",
    "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question #4:**\n",
    "\n",
    "The current `must_mention` metric has several limitations that could be improved:\n",
    "\n",
    "**Current Gaps:**\n",
    "1. **Case sensitivity** - The current implementation is case-sensitive, so \"peng\" wouldn't match \"Peng\"\n",
    "2. **Exact string matching** - It looks for exact substring matches, missing variations like \"Dr. Peng\" vs \"Peng\"\n",
    "3. **Binary scoring** - Returns 1.0 only if ALL phrases are found, 0.0 otherwise (no partial credit)\n",
    "4. **Context ignorance** - Doesn't consider if the phrase appears in the right context\n",
    "5. **Synonym/variation handling** - Misses related terms or different forms of names\n",
    "\n",
    "**Potential Improvements:**\n",
    "1. **Case-insensitive matching**: `phrase.lower() in outputs[\"answer\"].lower()`\n",
    "2. **Fuzzy matching**: Use libraries like `fuzzywuzzy` for approximate string matching\n",
    "3. **Partial scoring**: Return the percentage of required phrases found instead of binary\n",
    "4. **Semantic similarity**: Use embeddings to match semantically similar phrases\n",
    "5. **Named entity recognition**: Use NLP libraries to identify entities rather than exact strings\n",
    "6. **Regular expressions**: Handle variations in names/terms more flexibly\n",
    "7. **Context awareness**: Check if phrases appear in relevant contexts, not just anywhere\n",
    "\n",
    "**Example improved version:**\n",
    "```python\n",
    "def improved_must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "    required = reference_outputs.get(\"must_mention\") or []\n",
    "    answer = outputs[\"answer\"].lower()\n",
    "    found_count = 0\n",
    "    \n",
    "    for phrase in required:\n",
    "        if phrase.lower() in answer:\n",
    "            found_count += 1\n",
    "    \n",
    "    return found_count / len(required) if required else 1.0  # Partial scoring\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJr349zhv7"
   },
   "source": [
    "Task 3: Evaluating\n",
    "\n",
    "All that is left to do is evaluate our agent's response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "efcf57067cf743d8b4ce059a61cbe02e",
      "53e33aae3b97490c82aec7bbb0d6ebba",
      "ad84e0e971d3455db2efe7dd0d1f803e",
      "72adef9b70dd48198b7322b6c5b113cf",
      "8a61d045ffd44ac58f3f13eb10044836",
      "041e22a9b5514e36bd4d1dac01d5d398",
      "886d762f2a7c421382efb5502c6d42a1",
      "ab91fd625bbd43afbf8c6398193a88d0",
      "716557ad09874dcb989d75f7c74424cd",
      "77d4c0ebaae045b58efc4f789c9a2360",
      "0d622ccc56264fac8fd7508dbdbe6e29"
     ]
    },
    "id": "p5TeCUUkuGld",
    "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'simple_agent, baseline-4085d794' at:\n",
      "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/cd9ccfce-ebdd-4b2b-8c53-53df5a7a341b/compare?selectedSessions=6e4899d9-d28a-4008-8ffc-3e7883e9ef07\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9819a2b30c94b62b2a4cb83c615c784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = client.evaluate(\n",
    "    agent_chain_with_formatting,\n",
    "    data=dataset.name,\n",
    "    evaluators=[correctness_evaluator, must_mention],\n",
    "    experiment_prefix=\"simple_agent, baseline\",  # optional, experiment name prefix\n",
    "    description=\"Testing the baseline system.\",  # optional, experiment description\n",
    "    max_concurrency=4, # optional, add concurrency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTNe4kWrplB"
   },
   "source": [
    "## Part 2: LangGraph with Helpfulness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1wKRddbIY_S"
   },
   "source": [
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
    "\n",
    "We're going to make a few key adjustments to account for this:\n",
    "\n",
    "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
    "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npTYJ8ayR5B3"
   },
   "source": [
    "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-LQ84YhyJG0w"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD7EV0HqSQcb"
   },
   "source": [
    "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oajBwLkFVi1N"
   },
   "source": [
    "#### üèóÔ∏è Activity #4:\n",
    "\n",
    "Please write markdown for the following cells to explain what each is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6rN7feNVn9f"
   },
   "source": [
    "##### Creating the Graph with Helpfulness Check\n",
    "\n",
    "This cell initializes a new StateGraph with the AgentState and adds the same two nodes as before: the agent node (which calls the model) and the action node (which executes tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6r6XXA5FJbVf",
    "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ22o2mWVrfp"
   },
   "source": [
    "##### Setting the Entry Point\n",
    "\n",
    "This sets the agent node as the entry point, meaning the graph will start execution from the agent node when invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNWHwWxuRiLY",
    "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsXeF6xlaXOZ"
   },
   "source": [
    "##### Defining the Conditional Logic with Helpfulness Check\n",
    "\n",
    "This function implements the core logic for determining the next step in the graph. It checks if the last message contains tool calls (indicating the agent wants to use a tool), implements a loop limit (max 10 messages), and uses an LLM to evaluate if the response is helpful enough to end the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z_Sq3A9SaV1O"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def tool_call_or_helpful(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  initial_query = state[\"messages\"][0]\n",
    "  final_response = state[\"messages\"][-1]\n",
    "\n",
    "  if len(state[\"messages\"]) > 10:\n",
    "    return \"END\"\n",
    "\n",
    "  prompt_template = \"\"\"\\\n",
    "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
    "\n",
    "  Initial Query:\n",
    "  {initial_query}\n",
    "\n",
    "  Final Response:\n",
    "  {final_response}\"\"\"\n",
    "\n",
    "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
    "\n",
    "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
    "\n",
    "  if \"Y\" in helpfulness_response:\n",
    "    return \"end\"\n",
    "  else:\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BhnBW2YVsJO"
   },
   "source": [
    "##### Adding the Conditional Edge\n",
    "\n",
    "This adds a conditional edge from the agent node that can route to three different destinations: continue (back to agent for improvement), action (to execute tools), or end (to finish the conversation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVTKnWMbP_8T",
    "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tool_call_or_helpful,\n",
    "    {\n",
    "        \"continue\" : \"agent\",\n",
    "        \"action\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGDLEWOIVtK0"
   },
   "source": [
    "##### Adding the Tool-to-Agent Edge\n",
    "\n",
    "This creates a direct edge from the action node back to the agent node, ensuring that after any tool execution, the flow returns to the agent for processing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbDK2MbuREgU",
    "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSI8AOaEVvT-"
   },
   "source": [
    "##### Compiling the Graph\n",
    "\n",
    "This compiles the graph into an executable workflow that can be invoked like any other LangChain runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oQldl8ERQ8lf"
   },
   "outputs": [],
   "source": [
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F67FGCMRVwGz"
   },
   "source": [
    "##### Testing the Enhanced Agent\n",
    "\n",
    "This cell tests the agent with helpfulness checking by asking about \"Deep Research Agents\" and streaming the responses to see how the agent behaves with the new helpfulness evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3oo8E-PRK1T",
    "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Deep Research Agents are advanced AI systems designed to assist with in-depth research tasks. They leverage deep learning techniques and large datasets to analyze complex information, generate insights, and support decision-making across various fields such as science, technology, medicine, and more. These agents can automate literature reviews, extract relevant data from vast sources, and provide comprehensive summaries, making research more efficient and thorough. Would you like me to find more detailed or specific information about Deep Research Agents?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 158, 'total_tokens': 251, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJ4p3pIKceAjbeS3g3QtsJLgucObk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b10b2082-4074-4d48-848f-37d11d18949b-0', usage_metadata={'input_tokens': 158, 'output_tokens': 93, 'total_tokens': 251, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"What are Deep Research Agents?\")]}\n",
    "\n",
    "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVmZPs6lnpsM"
   },
   "source": [
    "## Part 3: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "### Task 4: Helpfulness Check of Gen AI Pattern Descriptions\n",
    "\n",
    "Let's ask our system about the 3 main patterns in Generative AI:\n",
    "\n",
    "1. Context Engineering\n",
    "2. Fine-tuning\n",
    "3. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZoLl7GlXoae-"
   },
   "outputs": [],
   "source": [
    "patterns = [\"Context Engineering\", \"Fine-tuning\", \"LLM-based agents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkh0YJuCp3Zl",
    "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Engineering is a relatively new interdisciplinary field that focuses on designing, managing, and optimizing the context in which systems, especially artificial intelligence and software applications, operate. It involves understanding and shaping the environment, circumstances, and background information that influence how systems behave and interact with users. The goal is to improve system performance, user experience, and decision-making by carefully engineering the context.\n",
      "\n",
      "The concept of Context Engineering has gained prominence with the rise of AI, IoT, and complex software systems, where context plays a crucial role in system effectiveness. It started to break onto the scene in the late 2010s and early 2020s, driven by advancements in contextual AI, ubiquitous computing, and the need for more adaptive and personalized systems.\n",
      "\n",
      "Would you like me to find more detailed or specific information about its origins and development?\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuning is a machine learning technique used to adapt a pre-trained model to a specific task or dataset. Instead of training a model from scratch, which can be resource-intensive and time-consuming, fine-tuning involves taking an existing model that has already learned general features from a large dataset and then further training it on a smaller, task-specific dataset. This process helps the model specialize and improve its performance on the target task while leveraging the knowledge it has already acquired.\n",
      "\n",
      "Fine-tuning has become a prominent approach in the development of large language models, computer vision models, and other deep learning architectures. It allows researchers and developers to efficiently customize models for various applications, such as natural language processing, image recognition, and more.\n",
      "\n",
      "As for when it \"broke onto the scene,\" fine-tuning has been around in various forms for many years, but it gained widespread popularity and recognition with the rise of large pre-trained models like BERT (introduced in 2018) and GPT (with GPT-2 in 2019 and GPT-3 in 2020). These models demonstrated the effectiveness of transfer learning and fine-tuning, leading to a significant shift in how AI models are developed and deployed.\n",
      "\n",
      "Would you like me to find more detailed historical information or specific milestones related to the development of fine-tuning?\n",
      "\n",
      "\n",
      "\n",
      "LLM-based agents are intelligent systems that leverage large language models (LLMs) to perform a variety of tasks, such as understanding natural language, generating human-like responses, and making decisions or taking actions based on the input they receive. These agents can be used in applications like chatbots, virtual assistants, automated customer support, and more complex decision-making systems.\n",
      "\n",
      "The concept of LLM-based agents gained significant attention and broke onto the scene around 2020-2021, coinciding with the development and release of advanced large language models like OpenAI's GPT-3 in 2020. GPT-3's impressive capabilities demonstrated the potential of LLMs to serve as foundational components for building intelligent agents that can understand and generate human language at a high level.\n",
      "\n",
      "Since then, the field has rapidly evolved, with numerous innovations in model architecture, training techniques, and applications, making LLM-based agents a prominent area of research and development in artificial intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pattern in patterns:\n",
    "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
    "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
    "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
    "  print(messages[\"messages\"][-1].content)\n",
    "  print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
